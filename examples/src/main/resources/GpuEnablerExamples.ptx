//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21554848
// Cuda compilation tools, release 8.0, V8.0.61
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_35
.address_size 64

	// .globl	multiplyBy2
.extern .func __assertfail
(
	.param .b64 __assertfail_param_0,
	.param .b64 __assertfail_param_1,
	.param .b32 __assertfail_param_2,
	.param .b64 __assertfail_param_3,
	.param .b64 __assertfail_param_4
)
;
.global .align 1 .b8 __T20[44] = {118, 111, 105, 100, 32, 115, 117, 109, 40, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 41, 0};
.global .align 1 .b8 __T21[47] = {118, 111, 105, 100, 32, 115, 117, 109, 108, 40, 105, 110, 116, 32, 42, 44, 32, 108, 111, 110, 103, 32, 42, 44, 32, 108, 111, 110, 103, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 41, 0};
.global .align 1 .b8 $str[31] = {106, 117, 109, 112, 32, 61, 61, 32, 98, 108, 111, 99, 107, 68, 105, 109, 46, 120, 32, 42, 32, 103, 114, 105, 100, 68, 105, 109, 46, 120, 0};
.global .align 1 .b8 $str1[22] = {71, 112, 117, 69, 110, 97, 98, 108, 101, 114, 69, 120, 97, 109, 112, 108, 101, 115, 46, 99, 117, 0};

.visible .entry multiplyBy2(
	.param .u64 multiplyBy2_param_0,
	.param .u64 multiplyBy2_param_1,
	.param .u64 multiplyBy2_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd3, [multiplyBy2_param_0];
	ld.param.u64 	%rd1, [multiplyBy2_param_1];
	ld.param.u64 	%rd2, [multiplyBy2_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r1, %r2, %r3, %r4;
	ld.global.u32 	%r5, [%rd4];
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB0_2;

	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%r6, [%rd7];
	shl.b32 	%r7, %r6, 1;
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd9, %rd8, %rd6;
	st.global.u32 	[%rd9], %r7;

BB0_2:
	ret;
}

	// .globl	multiplyBy2l
.visible .entry multiplyBy2l(
	.param .u64 multiplyBy2l_param_0,
	.param .u64 multiplyBy2l_param_1,
	.param .u64 multiplyBy2l_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd3, [multiplyBy2l_param_0];
	ld.param.u64 	%rd1, [multiplyBy2l_param_1];
	ld.param.u64 	%rd2, [multiplyBy2l_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r1, %r2, %r3, %r4;
	ld.global.u32 	%r5, [%rd4];
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB1_2;

	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u64 	%rd8, [%rd7];
	shl.b64 	%rd9, %rd8, 1;
	cvta.to.global.u64 	%rd10, %rd2;
	add.s64 	%rd11, %rd10, %rd6;
	st.global.u64 	[%rd11], %rd9;

BB1_2:
	ret;
}

	// .globl	sum
.visible .entry sum(
	.param .u64 sum_param_0,
	.param .u64 sum_param_1,
	.param .u64 sum_param_2,
	.param .u64 sum_param_3,
	.param .u64 sum_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<43>;


	ld.param.u64 	%rd16, [sum_param_0];
	ld.param.u64 	%rd17, [sum_param_1];
	ld.param.u64 	%rd15, [sum_param_2];
	ld.param.u64 	%rd18, [sum_param_3];
	cvta.to.global.u64 	%rd39, %rd17;
	cvta.to.global.u64 	%rd2, %rd16;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd19, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd20, %r3, %r2;
	add.s64 	%rd3, %rd20, %rd19;
	cvta.to.global.u64 	%rd21, %rd18;
	ld.global.u32 	%r13, [%rd21];
	setp.eq.s32	%p1, %r13, 0;
	@%p1 bra 	BB2_5;

	setp.ne.s64	%p2, %rd3, 0;
	@%p2 bra 	BB2_12;

	ld.global.s32 	%rd23, [%rd2];
	setp.lt.s64	%p3, %rd23, 16384;
	selp.b64	%rd4, %rd23, 16384, %p3;
	mov.u32 	%r24, 0;
	mov.u32 	%r25, %r24;
	mov.u64 	%rd40, 0;
	setp.lt.s64	%p4, %rd4, 1;
	@%p4 bra 	BB2_4;

BB2_3:
	ld.global.u32 	%r16, [%rd39];
	add.s32 	%r25, %r16, %r25;
	add.s64 	%rd39, %rd39, 4;
	add.s64 	%rd40, %rd40, 1;
	setp.lt.s64	%p5, %rd40, %rd4;
	mov.u32 	%r24, %r25;
	@%p5 bra 	BB2_3;

BB2_4:
	cvta.to.global.u64 	%rd24, %rd15;
	st.global.u32 	[%rd24], %r24;
	bra.uni 	BB2_12;

BB2_5:
	ld.global.u32 	%r26, [%rd2];
	cvt.s64.s32	%rd25, %r26;
	setp.ge.s64	%p6, %rd3, %rd25;
	@%p6 bra 	BB2_12;

	mov.u32 	%r17, %nctaid.x;
	mul.lo.s32 	%r18, %r17, %r3;
	setp.eq.s32	%p7, %r18, 16384;
	@%p7 bra 	BB2_8;

	mov.u64 	%rd26, $str;
	cvta.global.u64 	%rd27, %rd26;
	mov.u64 	%rd28, $str1;
	cvta.global.u64 	%rd29, %rd28;
	mov.u64 	%rd30, __T20;
	cvta.global.u64 	%rd31, %rd30;
	mov.u32 	%r19, 31;
	mov.u64 	%rd32, 1;
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd27;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd29;
	.param .b32 param2;
	st.param.b32	[param2+0], %r19;
	.param .b64 param3;
	st.param.b64	[param3+0], %rd31;
	.param .b64 param4;
	st.param.b64	[param4+0], %rd32;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	
	//{
	}// Callseq End 0
	ld.global.u32 	%r26, [%rd2];

BB2_8:
	cvt.s64.s32	%rd9, %r26;
	mov.u32 	%r27, 0;
	setp.ge.s64	%p8, %rd3, %rd9;
	@%p8 bra 	BB2_11;

	add.s64 	%rd35, %rd20, %rd19;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd41, %rd39, %rd36;
	mov.u32 	%r27, 0;
	mov.u64 	%rd42, %rd3;

BB2_10:
	mov.u64 	%rd12, %rd42;
	ld.global.u32 	%r22, [%rd41];
	add.s32 	%r27, %r22, %r27;
	add.s64 	%rd41, %rd41, 65536;
	add.s64 	%rd14, %rd12, 16384;
	setp.lt.s64	%p9, %rd14, %rd9;
	mov.u64 	%rd42, %rd14;
	@%p9 bra 	BB2_10;

BB2_11:
	shl.b64 	%rd37, %rd3, 2;
	add.s64 	%rd38, %rd39, %rd37;
	st.global.u32 	[%rd38], %r27;

BB2_12:
	ret;
}

	// .globl	suml
.visible .entry suml(
	.param .u64 suml_param_0,
	.param .u64 suml_param_1,
	.param .u64 suml_param_2,
	.param .u64 suml_param_3,
	.param .u64 suml_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<59>;


	ld.param.u64 	%rd22, [suml_param_0];
	ld.param.u64 	%rd23, [suml_param_1];
	ld.param.u64 	%rd21, [suml_param_2];
	ld.param.u64 	%rd24, [suml_param_3];
	cvta.to.global.u64 	%rd51, %rd23;
	cvta.to.global.u64 	%rd2, %rd22;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd25, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd26, %r3, %r2;
	add.s64 	%rd3, %rd26, %rd25;
	cvta.to.global.u64 	%rd27, %rd24;
	ld.global.u32 	%r7, [%rd27];
	setp.eq.s32	%p1, %r7, 0;
	@%p1 bra 	BB3_5;

	setp.ne.s64	%p2, %rd3, 0;
	@%p2 bra 	BB3_12;

	ld.global.s32 	%rd31, [%rd2];
	setp.lt.s64	%p3, %rd31, 16384;
	selp.b64	%rd4, %rd31, 16384, %p3;
	mov.u64 	%rd54, 0;
	mov.u64 	%rd55, %rd54;
	mov.u64 	%rd52, %rd54;
	setp.lt.s64	%p4, %rd4, 1;
	@%p4 bra 	BB3_4;

BB3_3:
	ld.global.u64 	%rd32, [%rd51];
	add.s64 	%rd55, %rd32, %rd55;
	add.s64 	%rd51, %rd51, 8;
	add.s64 	%rd52, %rd52, 1;
	setp.lt.s64	%p5, %rd52, %rd4;
	mov.u64 	%rd54, %rd55;
	@%p5 bra 	BB3_3;

BB3_4:
	cvta.to.global.u64 	%rd33, %rd21;
	st.global.u64 	[%rd33], %rd54;
	bra.uni 	BB3_12;

BB3_5:
	ld.global.u32 	%r11, [%rd2];
	cvt.s64.s32	%rd34, %r11;
	setp.ge.s64	%p6, %rd3, %rd34;
	@%p6 bra 	BB3_12;

	mov.u32 	%r8, %nctaid.x;
	mul.lo.s32 	%r9, %r8, %r3;
	setp.eq.s32	%p7, %r9, 16384;
	@%p7 bra 	BB3_8;

	mov.u64 	%rd35, $str;
	cvta.global.u64 	%rd36, %rd35;
	mov.u64 	%rd37, $str1;
	cvta.global.u64 	%rd38, %rd37;
	mov.u64 	%rd39, __T21;
	cvta.global.u64 	%rd40, %rd39;
	mov.u32 	%r10, 56;
	mov.u64 	%rd41, 1;
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd36;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd38;
	.param .b32 param2;
	st.param.b32	[param2+0], %r10;
	.param .b64 param3;
	st.param.b64	[param3+0], %rd40;
	.param .b64 param4;
	st.param.b64	[param4+0], %rd41;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	
	//{
	}// Callseq End 1
	ld.global.u32 	%r11, [%rd2];

BB3_8:
	cvt.s64.s32	%rd12, %r11;
	mov.u64 	%rd58, 0;
	setp.ge.s64	%p8, %rd3, %rd12;
	@%p8 bra 	BB3_11;

	add.s64 	%rd46, %rd26, %rd25;
	shl.b64 	%rd47, %rd46, 3;
	add.s64 	%rd56, %rd51, %rd47;
	mov.u64 	%rd58, 0;
	mov.u64 	%rd57, %rd3;

BB3_10:
	mov.u64 	%rd15, %rd57;
	ld.global.u64 	%rd48, [%rd56];
	add.s64 	%rd58, %rd48, %rd58;
	add.s64 	%rd56, %rd56, 131072;
	add.s64 	%rd19, %rd15, 16384;
	setp.lt.s64	%p9, %rd19, %rd12;
	mov.u64 	%rd57, %rd19;
	@%p9 bra 	BB3_10;

BB3_11:
	shl.b64 	%rd49, %rd3, 3;
	add.s64 	%rd50, %rd51, %rd49;
	st.global.u64 	[%rd50], %rd58;

BB3_12:
	ret;
}

	// .globl	sum1
.visible .entry sum1(
	.param .u64 sum1_param_0,
	.param .u64 sum1_param_1,
	.param .u64 sum1_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd7, [sum1_param_0];
	ld.param.u64 	%rd8, [sum1_param_1];
	ld.param.u64 	%rd9, [sum1_param_2];
	mov.u32 	%r5, %tid.x;
	cvt.u64.u32	%rd10, %r5;
	mov.u32 	%r6, %ctaid.x;
	cvt.u64.u32	%rd11, %r6;
	mov.u32 	%r7, %ntid.x;
	cvt.u64.u32	%rd12, %r7;
	neg.s64 	%rd13, %rd11;
	mul.lo.s64 	%rd14, %rd12, %rd13;
	setp.ne.s64	%p1, %rd10, %rd14;
	@%p1 bra 	BB4_5;

	cvta.to.global.u64 	%rd15, %rd7;
	ld.global.u32 	%r1, [%rd15];
	mov.u32 	%r11, 0;
	setp.lt.s32	%p2, %r1, 1;
	@%p2 bra 	BB4_4;

	cvta.to.global.u64 	%rd18, %rd8;
	cvt.s64.s32	%rd2, %r1;
	mov.u32 	%r11, 0;
	mov.u64 	%rd19, 0;

BB4_3:
	ld.global.u32 	%r10, [%rd18];
	add.s32 	%r11, %r10, %r11;
	add.s64 	%rd18, %rd18, 4;
	add.s64 	%rd19, %rd19, 1;
	setp.lt.s64	%p3, %rd19, %rd2;
	@%p3 bra 	BB4_3;

BB4_4:
	cvta.to.global.u64 	%rd17, %rd9;
	st.global.u32 	[%rd17], %r11;

BB4_5:
	ret;
}


