//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21554848
// Cuda compilation tools, release 8.0, V8.0.61
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_35
.address_size 64

	// .globl	invokeBlockSumKernel
// invokeBlockSumKernel$__cuda_local_var_51746_64_non_const_temp_storage has been demoted

.visible .entry invokeBlockSumKernel(
	.param .u32 invokeBlockSumKernel_param_0,
	.param .u64 invokeBlockSumKernel_param_1,
	.param .u64 invokeBlockSumKernel_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<15>;
	// demoted variable
	.shared .align 4 .b8 invokeBlockSumKernel$__cuda_local_var_51746_64_non_const_temp_storage[16];

	ld.param.u64 	%rd3, [invokeBlockSumKernel_param_1];
	ld.param.u64 	%rd2, [invokeBlockSumKernel_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd5, %r1;
	mov.u32 	%r26, %ctaid.x;
	mov.u32 	%r27, %ntid.x;
	mul.wide.u32 	%rd6, %r27, %r26;
	add.s64 	%rd1, %rd6, %rd5;
	cvt.s64.s32 	%rd7, %rd1;
	shl.b64 	%rd8, %rd7, 3;
	add.s64 	%rd9, %rd4, %rd8;
	ld.global.u32 	%r28, [%rd9];
	ld.global.u32 	%r29, [%rd9+512];
	ld.global.u32 	%r30, [%rd9+1024];
	ld.global.u32 	%r31, [%rd9+1536];
	ld.global.u32 	%r32, [%rd9+2048];
	ld.global.u32 	%r33, [%rd9+2560];
	ld.global.u32 	%r34, [%rd9+3072];
	ld.global.u32 	%r35, [%rd9+3584];
	ld.global.u32 	%r36, [%rd9+4096];
	ld.global.u32 	%r37, [%rd9+4608];
	ld.global.u32 	%r38, [%rd9+5120];
	ld.global.u32 	%r39, [%rd9+5632];
	ld.global.u32 	%r40, [%rd9+6144];
	ld.global.u32 	%r41, [%rd9+6656];
	ld.global.u32 	%r42, [%rd9+7168];
	ld.global.u32 	%r43, [%rd9+7680];
	add.s32 	%r44, %r29, %r28;
	add.s32 	%r45, %r44, %r30;
	add.s32 	%r46, %r45, %r31;
	add.s32 	%r47, %r46, %r32;
	add.s32 	%r48, %r47, %r33;
	add.s32 	%r49, %r48, %r34;
	add.s32 	%r50, %r49, %r35;
	add.s32 	%r51, %r50, %r36;
	add.s32 	%r52, %r51, %r37;
	add.s32 	%r53, %r52, %r38;
	add.s32 	%r54, %r53, %r39;
	add.s32 	%r55, %r54, %r40;
	add.s32 	%r56, %r55, %r41;
	add.s32 	%r57, %r56, %r42;
	add.s32 	%r7, %r57, %r43;
	// inline asm
	mov.u32 %r4, %laneid;
	// inline asm
	// inline asm
	mov.u32 %r5, %laneid;
	// inline asm
	mov.u32 	%r8, 1;
	mov.u32 	%r25, 31;
	// inline asm
	shfl.down.b32 %r6, %r7, %r8, %r25;
	// inline asm
	add.s32 	%r58, %r5, 1;
	setp.lt.s32	%p1, %r58, 32;
	selp.b32	%r59, %r6, 0, %p1;
	add.s32 	%r11, %r59, %r7;
	mov.u32 	%r12, 2;
	// inline asm
	shfl.down.b32 %r10, %r11, %r12, %r25;
	// inline asm
	add.s32 	%r60, %r5, 2;
	setp.lt.s32	%p2, %r60, 32;
	selp.b32	%r61, %r10, 0, %p2;
	add.s32 	%r15, %r11, %r61;
	mov.u32 	%r16, 4;
	// inline asm
	shfl.down.b32 %r14, %r15, %r16, %r25;
	// inline asm
	add.s32 	%r62, %r5, 4;
	setp.lt.s32	%p3, %r62, 32;
	selp.b32	%r63, %r14, 0, %p3;
	add.s32 	%r19, %r15, %r63;
	mov.u32 	%r20, 8;
	// inline asm
	shfl.down.b32 %r18, %r19, %r20, %r25;
	// inline asm
	add.s32 	%r64, %r5, 8;
	setp.lt.s32	%p4, %r64, 32;
	selp.b32	%r65, %r18, 0, %p4;
	add.s32 	%r23, %r19, %r65;
	mov.u32 	%r24, 16;
	// inline asm
	shfl.down.b32 %r22, %r23, %r24, %r25;
	// inline asm
	add.s32 	%r66, %r5, 16;
	setp.lt.s32	%p5, %r66, 32;
	selp.b32	%r67, %r22, 0, %p5;
	add.s32 	%r2, %r23, %r67;
	setp.ne.s32	%p6, %r4, 0;
	@%p6 bra 	BB0_2;

	shr.u32 	%r68, %r1, 5;
	mul.wide.u32 	%rd10, %r68, 4;
	mov.u64 	%rd11, invokeBlockSumKernel$__cuda_local_var_51746_64_non_const_temp_storage;
	add.s64 	%rd12, %rd11, %rd10;
	st.shared.u32 	[%rd12+4], %r2;

BB0_2:
	bar.sync 	0;
	ld.shared.u32 	%r69, [invokeBlockSumKernel$__cuda_local_var_51746_64_non_const_temp_storage+8];
	setp.eq.s32	%p7, %r1, 0;
	selp.b32	%r70, %r69, 0, %p7;
	add.s32 	%r3, %r70, %r2;
	setp.ne.s64	%p8, %rd1, 0;
	@%p8 bra 	BB0_4;

	cvta.to.global.u64 	%rd13, %rd2;
	cvt.s64.s32	%rd14, %r3;
	st.global.u64 	[%rd13], %rd14;

BB0_4:
	ret;
}

	// .globl	_ZN3cub11EmptyKernelIvEEvv
.visible .entry _ZN3cub11EmptyKernelIvEEvv(

)
{



	ret;
}


